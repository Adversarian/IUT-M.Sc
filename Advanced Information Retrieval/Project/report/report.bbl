% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{noauthor_what_nodate}
\BIBentryALTinterwordspacing
``\BIBforeignlanguage{en}{What is {Language} {Modeling}?}'' [Online].
  Available:
  \url{https://www.techtarget.com/searchenterpriseai/definition/language-modeling}
\BIBentrySTDinterwordspacing

\bibitem{fallah_overview_2021}
\BIBentryALTinterwordspacing
M.~Fallah, ``\BIBforeignlanguage{en}{An {Overview} of {Different}
  {Transformer}-based {Language} {Models}},'' Mar. 2021. [Online]. Available:
  \url{https://techblog.ezra.com/an-overview-of-different-transformer-based-language-models-c9d3adafead8}
\BIBentrySTDinterwordspacing

\bibitem{noauthor_language_nodate}
\BIBentryALTinterwordspacing
``Language {Modeling} with nn.{Transformer} and {TorchText} â€” {PyTorch}
  {Tutorials} 1.13.1+cu117 documentation.'' [Online]. Available:
  \url{https://pytorch.org/tutorials/beginner/transformer\_tutorial.html}
\BIBentrySTDinterwordspacing

\bibitem{noauthor_better_2019}
\BIBentryALTinterwordspacing
``\BIBforeignlanguage{en}{Better {Language} {Models} and {Their}
  {Implications}},'' Feb. 2019. [Online]. Available:
  \url{https://openai.com/blog/better-language-models/}
\BIBentrySTDinterwordspacing

\bibitem{noauthor_steganography_2019}
\BIBentryALTinterwordspacing
``\BIBforeignlanguage{en-US}{Steganography {Tutorial} {\textbar} {A} {Complete}
  {Guide} {For} {Beginners}},'' Jan. 2019. [Online]. Available:
  \url{https://www.edureka.co/blog/steganography-tutorial}
\BIBentrySTDinterwordspacing

\bibitem{prasad2011new}
R.~S.~R. Prasad and K.~Alla, ``A new approach to telugu text steganography,''
  in \emph{2011 IEEE Symposium on Wireless Technology and Applications
  (ISWTA)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2011, pp. 60--65.

\bibitem{bennett2004linguistic}
K.~Bennett, ``Linguistic steganography: Survey, analysis, and robustness
  concerns for hiding information in text,'' 2004.

\bibitem{hamdan2016ah4s}
A.~M. Hamdan and A.~Hamarsheh, ``Ah4s: an algorithm of text in text
  steganography using the structure of omega network,'' \emph{Security and
  Communication Networks}, vol.~9, no.~18, pp. 6004--6016, 2016.

\bibitem{yang2020gan}
Z.~Yang, N.~Wei, Q.~Liu, Y.~Huang, and Y.~Zhang, ``Gan-tstega: Text
  steganography based on generative adversarial networks,'' in \emph{Digital
  Forensics and Watermarking: 18th International Workshop, IWDW 2019, Chengdu,
  China, November 2--4, 2019, Revised Selected Papers 18}.\hskip 1em plus 0.5em
  minus 0.4em\relax Springer, 2020, pp. 18--31.

\bibitem{kang2020generative}
H.~Kang, H.~Wu, and X.~Zhang, ``Generative text steganography based on lstm
  network and attention mechanism with keywords,'' \emph{Electronic Imaging},
  vol. 2020, no.~4, pp. 291--1, 2020.

\bibitem{wolf-etal-2020-transformers}
\BIBentryALTinterwordspacing
T.~Wolf, L.~Debut, V.~Sanh, J.~Chaumond, C.~Delangue, A.~Moi, P.~Cistac,
  T.~Rault, R.~Louf, M.~Funtowicz, J.~Davison, S.~Shleifer, P.~von Platen,
  C.~Ma, Y.~Jernite, J.~Plu, C.~Xu, T.~L. Scao, S.~Gugger, M.~Drame, Q.~Lhoest,
  and A.~M. Rush, ``Transformers: State-of-the-art natural language
  processing,'' in \emph{Proceedings of the 2020 Conference on Empirical
  Methods in Natural Language Processing: System Demonstrations}.\hskip 1em
  plus 0.5em minus 0.4em\relax Online: Association for Computational
  Linguistics, Oct. 2020, pp. 38--45. [Online]. Available:
  \url{https://www.aclweb.org/anthology/2020.emnlp-demos.6}
\BIBentrySTDinterwordspacing

\bibitem{radford2019language}
A.~Radford, J.~Wu, R.~Child, D.~Luan, D.~Amodei, I.~Sutskever \emph{et~al.},
  ``Language models are unsupervised multitask learners,'' \emph{OpenAI blog},
  vol.~1, no.~8, p.~9, 2019.

\bibitem{chen2015microsoft}
X.~Chen, H.~Fang, T.-Y. Lin, R.~Vedantam, S.~Gupta, P.~Doll{\'a}r, and C.~L.
  Zitnick, ``Microsoft coco captions: Data collection and evaluation server,''
  \emph{arXiv preprint arXiv:1504.00325}, 2015.

\bibitem{noauthor_translation_nodate}
\BIBentryALTinterwordspacing
``Translation {Task} - {ACL} 2017 {Second} {Conference} on {Machine}
  {Translation}.'' [Online]. Available:
  \url{https://www.statmt.org/wmt17/translation-task.html}
\BIBentrySTDinterwordspacing

\end{thebibliography}
