
@article{logreg,
	title = {The {Origins} of {Logistic} {Regression}},
	issn = {1556-5068},
	url = {http://www.ssrn.com/abstract=360300},
	doi = {10.2139/ssrn.360300},
	language = {en},
	urldate = {2022-06-29},
	journal = {SSRN Electronic Journal},
	author = {Cramer, J.S.},
	year = {2003},
}

@misc{dataset,
	title = {Gender {Recognition} by {Voice}},
	url = {https://www.kaggle.com/datasets/primaryobjects/voicegender},
	abstract = {Identify a voice as male or female},
	language = {en},
	urldate = {2022-06-29},
}

@article{svm,
	title = {Support-vector networks},
	volume = {20},
	issn = {0885-6125, 1573-0565},
	url = {http://link.springer.com/10.1007/BF00994018},
	doi = {10.1007/BF00994018},
	language = {en},
	number = {3},
	urldate = {2022-06-29},
	journal = {Machine Learning},
	author = {Cortes, Corinna and Vapnik, Vladimir},
	month = sep,
	year = {1995},
	pages = {273--297},
}

@techreport{nb,
	title = {Bayes and {Naive} {Bayes} {Classifier}},
	url = {http://arxiv.org/abs/1404.0933},
	abstract = {The Bayesian Classification represents a supervised learning method as well as a statistical method for classification. Assumes an underlying probabilistic model and it allows us to capture uncertainty about the model in a principled way by determining probabilities of the outcomes. This Classification is named after Thomas Bayes (1702-1761), who proposed the Bayes Theorem. Bayesian classification provides practical learning algorithms and prior knowledge and observed data can be combined. Bayesian Classification provides a useful perspective for understanding and evaluating many learning algorithms. It calculates explicit probabilities for hypothesis and it is robust to noise in input data. In statistical classification the Bayes classifier minimies the probability of misclassification. That was a visual intuition for a simple case of the Bayes classifier, also called: 1)Idiot Bayes 2)Naive Bayes 3)Simple Bayes},
	number = {arXiv:1404.0933},
	urldate = {2022-06-29},
	institution = {arXiv},
	author = {{Vikramkumar} and B, Vijaykumar and {Trilochan}},
	month = apr,
	year = {2014},
	note = {arXiv:1404.0933 [cs]
type: article},
	keywords = {Computer Science - Machine Learning},
}

@article{id3dt,
	title = {Induction of decision trees},
	volume = {1},
	issn = {1573-0565},
	url = {https://doi.org/10.1007/BF00116251},
	doi = {10.1007/BF00116251},
	abstract = {The technology for building knowledge-based systems by inductive inference from examples has been demonstrated successfully in several practical applications. This paper summarizes an approach to synthesizing decision trees that has been used in a variety of systems, and it describes one such system, ID3, in detail. Results from recent studies show ways in which the methodology can be modified to deal with information that is noisy and/or incomplete. A reported shortcoming of the basic algorithm is discussed and two means of overcoming it are compared. The paper concludes with illustrations of current research directions.},
	language = {en},
	number = {1},
	urldate = {2022-06-29},
	journal = {Machine Learning},
	author = {Quinlan, J. R.},
	month = mar,
	year = {1986},
	keywords = {classification, induction, decision trees, information theory, knowledge acquisition, expert systems},
	pages = {81--106},
}

@article{randomforests,
	title = {Random {Forests}},
	volume = {45},
	issn = {1573-0565},
	url = {https://doi.org/10.1023/A:1010933404324},
	doi = {10.1023/A:1010933404324},
	abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund \& R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148–156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
	language = {en},
	number = {1},
	urldate = {2022-06-29},
	journal = {Machine Learning},
	author = {Breiman, Leo},
	month = oct,
	year = {2001},
	keywords = {classification, regression, ensemble},
	pages = {5--32},
}

@article{knn,
	title = {Discriminatory {Analysis}. {Nonparametric} {Discrimination}: {Consistency} {Properties}},
	volume = {57},
	issn = {03067734},
	shorttitle = {Discriminatory {Analysis}. {Nonparametric} {Discrimination}},
	url = {https://www.jstor.org/stable/1403797?origin=crossref},
	doi = {10.2307/1403797},
	number = {3},
	urldate = {2022-06-29},
	journal = {International Statistical Review / Revue Internationale de Statistique},
	author = {Fix, Evelyn and Hodges, J. L.},
	month = dec,
	year = {1989},
	pages = {238},
}

@inproceedings{adaboost,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {A desicion-theoretic generalization of on-line learning and an application to boosting},
	isbn = {9783540491958},
	doi = {10.1007/3-540-59119-2_166},
	abstract = {We consider the problem of dynamically apportioning resources among a set of options in a worst-case on-line framework. The model we study can be interpreted as a broad, abstract extension of the well-studied on-line prediction model to a general decision-theoretic setting. We show that the multiplicative weight-update rule of Littlestone and Warmuth [10] can be adapted to this mode yielding bounds that are slightly weaker in some cases, but applicable to a considerably more general class of learning problems. We show how the resulting learning algorithm can be applied to a variety of problems, including gambling, multiple-outcome prediction, repeated games and prediction of points in ℝn. We also show how the weight-update rule can be used to derive a new boosting algorithm which does not require prior knowledge about the performance of the weak learning algorithm.},
	language = {en},
	booktitle = {Computational {Learning} {Theory}},
	publisher = {Springer},
	author = {Freund, Yoav and Schapire, Robert E.},
	editor = {Vitányi, Paul},
	year = {1995},
	keywords = {Loss Function, Weak Hypothesis, Algorithm AdaBoost, Final Hypothesis, Cumulative Loss},
	pages = {23--37},
}

@techreport{sgdm,
	title = {An overview of gradient descent optimization algorithms},
	url = {http://arxiv.org/abs/1609.04747},
	abstract = {Gradient descent optimization algorithms, while increasingly popular, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by. This article aims to provide the reader with intuitions with regard to the behaviour of different algorithms that will allow her to put them to use. In the course of this overview, we look at different variants of gradient descent, summarize challenges, introduce the most common optimization algorithms, review architectures in a parallel and distributed setting, and investigate additional strategies for optimizing gradient descent.},
	number = {arXiv:1609.04747},
	urldate = {2022-06-29},
	institution = {arXiv},
	author = {Ruder, Sebastian},
	month = jun,
	year = {2017},
	note = {arXiv:1609.04747 [cs]
type: article},
	keywords = {Computer Science - Machine Learning},
}

@MISC {kinknn,
    TITLE = {Why is $k = \sqrt{N}$ a good solution of the number of neighbors to consider?},
    AUTHOR = {microhaus (https://stats.stackexchange.com/users/294515/microhaus)},
    HOWPUBLISHED = {Cross Validated},
    NOTE = {URL:https://stats.stackexchange.com/q/535051 (version: 2021-07-20)},
    EPRINT = {https://stats.stackexchange.com/q/535051},
    URL = {https://stats.stackexchange.com/q/535051}
}

@article{gini,
	title = {The {Gini} {Index} and {Measures} of {Inequality}},
	volume = {117},
	issn = {0002-9890},
	url = {https://www.jstor.org/stable/10.4169/000298910x523344},
	doi = {10.4169/000298910x523344},
	abstract = {Abstract The Gini index is a summary statistic that measures how equitably a resource is distributed in a population; income is a primary example. In addition to a self-contained presentation of the Gini index, we give two equivalent ways to interpret this summary statistic: first in terms of the percentile level of the person who earns the average dollar, and second in terms of how the lower of two randomly chosen incomes compares, on average, to mean income.},
	number = {10},
	urldate = {2022-06-30},
	journal = {The American Mathematical Monthly},
	author = {Farris, Frank A.},
	year = {2010},
	pages = {851--864},
}

@article{cart,
author = {Lewis, Roger},
year = {2000},
month = {01},
pages = {},
title = {An Introduction to Classification and Regression Tree (CART) Analysis}
}

@article{dtnpcom,
title = {Constructing optimal binary decision trees is NP-complete},
journal = {Information Processing Letters},
volume = {5},
number = {1},
pages = {15-17},
year = {1976},
issn = {0020-0190},
doi = {https://doi.org/10.1016/0020-0190(76)90095-8},
url = {https://www.sciencedirect.com/science/article/pii/0020019076900958},
author = {Laurent Hyafil and Ronald L. Rivest},
keywords = {Binary decision trees, computational complexity, NP-complete}
}

@ARTICLE{knnbounds,
  author={Cover, T. and Hart, P.},
  journal={IEEE Transactions on Information Theory}, 
  title={Nearest neighbor pattern classification}, 
  year={1967},
  volume={13},
  number={1},
  pages={21-27},
  doi={10.1109/TIT.1967.1053964}
}

@techreport{sklearn,
	title = {Scikit-learn: {Machine} {Learning} in {Python}},
	shorttitle = {Scikit-learn},
	url = {http://arxiv.org/abs/1201.0490},
	abstract = {Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.org.},
	number = {arXiv:1201.0490},
	urldate = {2022-06-30},
	institution = {arXiv},
	author = {Pedregosa, Fabian and Varoquaux, Gael and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Muller, Andreas and Nothman, Joel and Louppe, Gilles and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, Edouard},
	month = jun,
	year = {2018},
	note = {arXiv:1201.0490 [cs]
type: article},
	keywords = {Computer Science - Machine Learning, Computer Science - Mathematical Software},
}